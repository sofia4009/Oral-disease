{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sofia4009/Oral-disease/blob/main/Train_Test_GoogleDS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Pd-GO8Mt8Nk"
      },
      "source": [
        "Author: Sofia\n",
        "Date: March 26\n",
        "Subject: Training and getting experience with deep learning\n",
        "Classification of dataset from kaggle:  https://www.kaggle.com/datasets/salmansajid05/oral-diseases\n",
        "  - use stratified k-fold cross validation (with k = 5) to split dataset A\n",
        "  - use the following deep neural networks for training, testing and comparing their performance\n",
        "  - (do not use any data augmentation for now; just normalise all image pixel values to the range [0, 1];\n",
        "  - resize all images to dimensions 112x112x3 for speeding up the training;\n",
        "  - use categorical cross entropy as the loss function and f1 score as evaluation metric;\n",
        "  - use the pretrained models on ImageNet):\n",
        "    - ResNet18, ResNet50, ConvNeXt, EfficientNetB0, Transformers (i.e., ViT)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preparation"
      ],
      "metadata": {
        "id": "HoVsopVfrWh8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Vb8YBM1d2ULx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from openpyxl import load_workbook\n",
        "from openpyxl.drawing.image import Image as xlImage\n",
        "from openpyxl import Workbook\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "import h5py\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Yb5xs6kGsSvc"
      },
      "outputs": [],
      "source": [
        "# Initializations\n",
        "image_size = (112,112)\n",
        "\n",
        "# Specify the root directory where your images are located\n",
        "main_DS_directory = '/content/New_Dataset/DataSet B' #'/content/oral-diseases'\n",
        "\n",
        "# Specify the output directory to save the processed images\n",
        "Preprocessed_DS_directory = '/content/DataSet B' #'/content/decreased_oral_diseases'\n",
        "\n",
        "# Specify the output directory to save the excel file\n",
        "results_directory = '/content/drive/My Drive/QM/results.xlsx'\n",
        "Plot_directory = '/content/drive/My Drive/QM/plots.xlsx'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/QM/New_Dataset.zip') as zipObj:\n",
        "  members = [file for file in zipObj.namelist() if \"__MACOSX\" not in file]\n",
        "  zipObj.extractall('/content/New_Dataset', members=members)\n",
        "\n",
        "# Optionally, remove the __MACOSX directory if it was extracted\n",
        "macosx_folder = os.path.join('/content/New_Dataset', \"__MACOSX\")\n",
        "if os.path.exists(macosx_folder):\n",
        "    os.rmdir(macosx_folder)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNqIUhRHSXPR",
        "outputId": "096241bd-4279-4727-9ebe-4bb41bf7aff3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_wTWLizJQUsQ"
      },
      "outputs": [],
      "source": [
        "# Normalizing them to [0,1], Resizing to 112,112, 3\n",
        "def normalize_and_resize_image(image, target_size):\n",
        "\n",
        "    # Resize image into 112*112*3\n",
        "    image = image.resize(target_size)\n",
        "\n",
        "    # Normalize pixel values to the range [0, 1]\n",
        "    image = np.array(image)\n",
        "    image = image / 255.0\n",
        "\n",
        "    # Convert the normalized numpy array back to PIL image\n",
        "    image = Image.fromarray((image * 255).astype(np.uint8))\n",
        "\n",
        "    return image\n",
        "\n",
        "# Saving the normalized images into a new directory in Google colab with the same subdirectories and structure\n",
        "def process_images_in_directory(directory, Preprocessed_DS_directory):\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            # Check if the file has an image extension\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
        "                # Construct the full path to the input image file\n",
        "                image_path = os.path.join(root, file)\n",
        "                if 'Decay' not in image_path and 'Caries_Gingivitus_ToothDiscoloration_Ulcer-yolo_annotated-Dataset' not in image_path:\n",
        "                  # Open the image using PIL\n",
        "                  image = Image.open(image_path)\n",
        "\n",
        "                  # Ensure image is in RGB mode\n",
        "                  image = image.convert(\"RGB\")\n",
        "\n",
        "                  # Normalize and Resize the image\n",
        "                  #print(image_path)\n",
        "                  processed_image = normalize_and_resize_image(image, image_size)\n",
        "\n",
        "                  # Construct the full path to the output directory\n",
        "                  output_subdirectory = os.path.relpath(root, directory)\n",
        "                  output_path = os.path.join(Preprocessed_DS_directory, output_subdirectory)\n",
        "                  os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "                  # Save the processed image\n",
        "                  filename = os.path.join(output_path, file)\n",
        "                  processed_image.save(filename, format='JPEG')  # Change 'JPEG' to the desired format\n",
        "                  #print(filename)\n",
        "                  #if filename.find('augmented') == -1 or filename.find('Caries_Gingivitus_ToothDiscoloration_Ulcer-yolo_annotated-Dataset') == -1:\n",
        "                  #np.save(filename, image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2rUI1-BZ5fKF"
      },
      "outputs": [],
      "source": [
        "# Call the function to process images in the directory\n",
        "process_images_in_directory(main_DS_directory, Preprocessed_DS_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cbFoR3ExsAgi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34975431-b259-4b5a-b87c-38032cf468d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Original Images: 539\n",
            "Number of Preprocessed Images: 532\n"
          ]
        }
      ],
      "source": [
        "file_count = sum(len(files) for _, _, files in os.walk(main_DS_directory))\n",
        "print(f\"Number of Original Images: {file_count}\")\n",
        "\n",
        "file_count = sum(len(files) for _, _, files in os.walk(Preprocessed_DS_directory))\n",
        "print(f\"Number of Preprocessed Images: {file_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWRRMP-9odcR"
      },
      "source": [
        "# Training the dataset Using Pre-trained models on ImageNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4kGjPMiVLkU"
      },
      "source": [
        "# Initializations for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aMA_y9bi2YHZ"
      },
      "outputs": [],
      "source": [
        "# Define transformations by Composing several transforms together\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    #Convert a PIL Image or ndarray to tensor\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Use ImageFolder to load your dataset\n",
        "dataset = torchvision.datasets.ImageFolder(root = Preprocessed_DS_directory, transform=transform)\n",
        "\n",
        "# Define number of classes and list of labels in the dataset\n",
        "num_classes = len(dataset.classes)\n",
        "classes = dataset.classes\n",
        "\n",
        "# Define k-fold for cross-validation\n",
        "k_folds = 5\n",
        "\n",
        "# Seed (random_state) is set to initialize the random number generator while splitting the DataSet into k folds\n",
        "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# Define hyperparameters to search over\n",
        "learning_rates = [0.001, 0.0001]\n",
        "batch_sizes = [32, 64, 128]\n",
        "optimizers = ['Adam', 'SGD', 'RMSprop']\n",
        "\n",
        "epochs = 35\n",
        "\n",
        "best_model = None\n",
        "best_f1 = 0.0\n",
        "best_hyperparameters = None\n",
        "\n",
        "# Define the path where you want to save the model weights in Google Drive\n",
        "checkpoint_path = '/content/drive/My Drive/QM/modelDSBfold1.h5'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OuNTqEiG506H"
      },
      "outputs": [],
      "source": [
        "#print(classes)\n",
        "class_counts = {}\n",
        "for class_name in os.listdir(Preprocessed_DS_directory):\n",
        "    # Construct the full path to the class directory\n",
        "    class_directory = os.path.join(Preprocessed_DS_directory, class_name)\n",
        "\n",
        "    # Check if the path is a directory\n",
        "    if os.path.isdir(class_directory):\n",
        "        # Count the number of files in the class directory\n",
        "        num_files = len(os.listdir(class_directory))\n",
        "\n",
        "        # Store the count in the class_counts dictionary\n",
        "        class_counts[class_name] = num_files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNW54ARY62Mb"
      },
      "source": [
        "# ResNet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0n2uHvbcj-l"
      },
      "outputs": [],
      "source": [
        "\"\"\"# ResNet18\"\"\"\n",
        "\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "# An empty list for storing the results\n",
        "results = []\n",
        "f1_vals = [0] * k_folds\n",
        "results.append({'Model': 'ResNet18'})\n",
        "\n",
        "# Initialize lists to store training and validation losses and accuracies\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "epochs = 10\n",
        "num_epochs = epochs\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for batch_size in batch_sizes:\n",
        "      for optimizer_name in optimizers:\n",
        "        print(f\"Training with optimizer: {optimizer_name}, learning rate: {lr}, batch size: {batch_size}\")\n",
        "        for fold, (train_idx, val_idx) in enumerate(skf.split(dataset.imgs, dataset.targets)):\n",
        "            train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
        "            val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
        "\n",
        "            train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "            val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
        "\n",
        "            best_f1 = 0\n",
        "            # Define the device (GPU if available, otherwise CPU)\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            print(torch.cuda.is_available())\n",
        "\n",
        "            # Define the neural network\n",
        "            model = resnet18(weights='ResNet18_Weights.DEFAULT')\n",
        "            num_ftrs = model.fc.in_features\n",
        "            model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "            model = model.to(device) # Move the model to GPU\n",
        "\n",
        "            # Define loss function\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "            # Define optimizer\n",
        "            if optimizer_name == 'Adam':\n",
        "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "            elif optimizer_name == 'SGD':\n",
        "                optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "            elif optimizer_name == 'RMSprop':\n",
        "                optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
        "\n",
        "            # Train and evaluate\n",
        "            for epoch in range(num_epochs):\n",
        "                # Training loop\n",
        "                model.train()\n",
        "                running_loss = 0.0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "                for inputs, labels in train_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device) # Move inputs and labels to GPU\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    running_loss += loss.item()\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (preds == labels).sum().item()\n",
        "                epoch_train_loss = running_loss / len(train_loader)\n",
        "                epoch_train_accuracy = correct / total\n",
        "                train_losses.append(epoch_train_loss)\n",
        "                train_accuracies.append(epoch_train_accuracy)\n",
        "\n",
        "                # Validation loop\n",
        "                model.eval()\n",
        "                all_preds = []\n",
        "                all_labels = []\n",
        "                running_loss = 0.0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "                with torch.no_grad():\n",
        "                  for inputs, labels in val_loader:\n",
        "                      inputs, labels = inputs.to(device), labels.to(device) # Move inputs and labels to GPU\n",
        "                      outputs = model(inputs)\n",
        "                      loss = criterion(outputs, labels)\n",
        "                      running_loss += loss.item()\n",
        "                      _, preds = torch.max(outputs, 1)\n",
        "                      total += labels.size(0)\n",
        "                      correct += (preds == labels).sum().item()\n",
        "                      all_preds.extend(preds.cpu().numpy())\n",
        "                      all_labels.extend(labels.cpu().numpy())\n",
        "                epoch_val_loss = running_loss / len(val_loader)\n",
        "                epoch_val_accuracy = correct / total\n",
        "                val_losses.append(epoch_val_loss)\n",
        "                val_accuracies.append(epoch_val_accuracy)\n",
        "\n",
        "                # Calculate F1 score\n",
        "                f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "                print(f\"Fold {fold+1}, Epoch {epoch+1}, F1 Score: {f1}\")\n",
        "\n",
        "                # Update best F1 score and model\n",
        "                if f1 > best_f1:\n",
        "                  best_f1 = f1\n",
        "\n",
        "            f1_vals[fold] = best_f1\n",
        "            results.append({\n",
        "                    'Optimizer': optimizer_name,\n",
        "                    'batch_size': batch_size,\n",
        "                    'learning_rate': lr,\n",
        "                    'Fold': fold+1,\n",
        "                    'F1 Score': best_f1\n",
        "              })\n",
        "        results.append({\n",
        "                  'min f1': min(f1_vals),\n",
        "                  'max f1': max(f1_vals),\n",
        "                  'average f1': sum(f1_vals)/len(f1_vals)\n",
        "                  })\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Save the results in a excel file ---------------------\n",
        "# Save the results as a DataFrame to be saved in an Excel file\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Load an existing Excel file\n",
        "if os.path.isfile(results_directory):\n",
        "  existing_file = pd.read_excel(results_directory)\n",
        "  # Append the new DataFrame to the existing file\n",
        "  df = pd.concat([existing_file, df])\n",
        "\n",
        "# Write the DataFrame to the excel file in the directory\n",
        "df.to_excel(results_directory, index=False)\n",
        "# -------------- End of Save the results in a excel file ----------------\n",
        "\n",
        "# -------------------- Plot the error/epoch plot ------------------------\n",
        "\n",
        "# Define the filename for the Excel file\n",
        "excel_filename = Plot_directory\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(os.path.dirname(excel_filename), exist_ok=True)\n",
        "\n",
        "# Check if the Excel file exists\n",
        "if os.path.exists(excel_filename):\n",
        "    # Load existing Excel file\n",
        "    wb = load_workbook(excel_filename)\n",
        "    ws = wb.active\n",
        "else:\n",
        "    # Create a new Excel workbook\n",
        "    wb = Workbook()\n",
        "    ws = wb.active"
      ],
      "metadata": {
        "id": "PxIjoVnWqXiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8h6Q1Mp7NSP"
      },
      "source": [
        "#ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"#ResNet50\"\"\"\n",
        "\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# An empty list for storing the results\n",
        "results = []\n",
        "f1_vals = [0] * k_folds\n",
        "results.append({'Model': 'ResNet50'})\n",
        "\n",
        "# Initialize lists to store training and validation losses and accuracies\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "epochs = 10\n",
        "# Define hyperparameters to search over\n",
        "learning_rates = [0.0001]\n",
        "batch_sizes = [32]\n",
        "optimizers = ['Adam', 'SGD', 'RMSprop']\n",
        "num_epochs = epochs\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for batch_size in batch_sizes:\n",
        "      for optimizer_name in optimizers:\n",
        "        print(f\"Training with optimizer: {optimizer_name}, learning rate: {lr}, batch size: {batch_size}\")\n",
        "        for fold, (train_idx, val_idx) in enumerate(skf.split(dataset.imgs, dataset.targets)):\n",
        "            train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
        "            val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
        "\n",
        "            train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "            val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
        "\n",
        "            best_f1 = 0\n",
        "            # Define the device (GPU if available, otherwise CPU)\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "            # Define the neural network\n",
        "            model = resnet50(weights='ResNet50_Weights.DEFAULT')\n",
        "            num_ftrs = model.fc.in_features\n",
        "            model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "            model = model.to(device) # Move the model to GPU\n",
        "\n",
        "            # Define loss function\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "            # Define optimizer\n",
        "            if optimizer_name == 'Adam':\n",
        "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "            elif optimizer_name == 'SGD':\n",
        "                optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "            elif optimizer_name == 'RMSprop':\n",
        "                optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
        "\n",
        "            # Train and evaluate\n",
        "            for epoch in range(num_epochs):\n",
        "                # Training loop\n",
        "                model.train()\n",
        "                running_loss = 0.0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "                for inputs, labels in train_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device) # Move inputs and labels to GPU\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    running_loss += loss.item()\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (preds == labels).sum().item()\n",
        "                epoch_train_loss = running_loss / len(train_loader)\n",
        "                epoch_train_accuracy = correct / total\n",
        "                train_losses.append(epoch_train_loss)\n",
        "                train_accuracies.append(epoch_train_accuracy)\n",
        "\n",
        "                # Validation loop\n",
        "                model.eval()\n",
        "                all_preds = []\n",
        "                all_labels = []\n",
        "                running_loss = 0.0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "                with torch.no_grad():\n",
        "                  for inputs, labels in val_loader:\n",
        "                      inputs, labels = inputs.to(device), labels.to(device) # Move inputs and labels to GPU\n",
        "                      outputs = model(inputs)\n",
        "                      loss = criterion(outputs, labels)\n",
        "                      running_loss += loss.item()\n",
        "                      _, preds = torch.max(outputs, 1)\n",
        "                      total += labels.size(0)\n",
        "                      correct += (preds == labels).sum().item()\n",
        "                      all_preds.extend(preds.cpu().numpy())\n",
        "                      all_labels.extend(labels.cpu().numpy())\n",
        "                epoch_val_loss = running_loss / len(val_loader)\n",
        "                epoch_val_accuracy = correct / total\n",
        "                val_losses.append(epoch_val_loss)\n",
        "                val_accuracies.append(epoch_val_accuracy)\n",
        "\n",
        "                # Calculate F1 score\n",
        "                f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "                print(f\"Fold {fold+1}, Epoch {epoch+1}, F1 Score: {f1}\")\n",
        "\n",
        "                # Update best F1 score and model\n",
        "                if f1 > best_f1:\n",
        "                  best_f1 = f1\n",
        "\n",
        "            f1_vals[fold] = best_f1\n",
        "            results.append({\n",
        "                    'Optimizer': optimizer_name,\n",
        "                    'batch_size': batch_size,\n",
        "                    'learning_rate': lr,\n",
        "                    'Fold': fold+1,\n",
        "                    'F1 Score': best_f1\n",
        "              })\n",
        "        results.append({\n",
        "                  'min f1': min(f1_vals),\n",
        "                  'max f1': max(f1_vals),\n",
        "                  'average f1': sum(f1_vals)/len(f1_vals)\n",
        "                  })"
      ],
      "metadata": {
        "id": "8c3arUzQBRMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Save the results in a excel file ---------------------\n",
        "# Save the results as a DataFrame to be saved in an Excel file\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Load an existing Excel file\n",
        "if os.path.isfile(results_directory):\n",
        "  existing_file = pd.read_excel(results_directory)\n",
        "  # Append the new DataFrame to the existing file\n",
        "  df = pd.concat([existing_file, df])\n",
        "\n",
        "# Write the DataFrame to the excel file in the directory\n",
        "df.to_excel(results_directory, index=False)\n",
        "# -------------- End of Save the results in a excel file ----------------\n",
        "\n",
        "# -------------------- Plot the error/epoch plot ------------------------\n",
        "\n",
        "# Define the filename for the Excel file\n",
        "excel_filename = Plot_directory\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(os.path.dirname(excel_filename), exist_ok=True)\n",
        "\n",
        "# Check if the Excel file exists\n",
        "if os.path.exists(excel_filename):\n",
        "    # Load existing Excel file\n",
        "    wb = load_workbook(excel_filename)\n",
        "    ws = wb.active\n",
        "else:\n",
        "    # Create a new Excel workbook\n",
        "    wb = Workbook()\n",
        "    ws = wb.active"
      ],
      "metadata": {
        "id": "DA9njfPoTbkj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c7e33ef-4683-4933-c344-08d1b025b866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with optimizer: Adam, learning rate: 0.0001, batch size: 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgEuugVdHf4S"
      },
      "source": [
        "# ConvNeXt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# ConvNeXt\"\"\"\n",
        "\n",
        "# An empty list for storing the results\n",
        "results = []\n",
        "f1_vals = [0] * k_folds\n",
        "results.append({'Model': 'ConvNeXt'})\n",
        "\n",
        "# Initialize lists to store training and validation losses and accuracies\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "num_epochs = epochs\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for batch_size in batch_sizes:\n",
        "      for optimizer_name in optimizers:\n",
        "        print(\"Training with optimizer: \" + optimizer_name + \", learning rate: \" + str(lr) + \", batch size: \" + str(batch_size))\n",
        "\n",
        "#        print(f\"Training with optimizer: {optimizer_name}, learning rate: {lr}, batch size: {batch_size}\")\n",
        "        for fold, (train_idx, val_idx) in enumerate(skf.split(dataset.imgs, dataset.targets)):\n",
        "            train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
        "            val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
        "\n",
        "            #train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "            #val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
        "\n",
        "            train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, pin_memory=True)\n",
        "            val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=val_sampler, pin_memory=True)\n",
        "\n",
        "            best_f1 = 0\n",
        "\n",
        "            # Define the neural network\n",
        "\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            model = torchvision.models.convnext_small(input_shape=image_size, num_classes=num_classes).to(device)\n",
        "\n",
        "            #model = torchvision.models.convnext_small(input_shape=image_size, num_classes=num_classes)\n",
        "\n",
        "            # Define loss function\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "            # Define optimizer\n",
        "            if optimizer_name == 'Adam':\n",
        "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "            elif optimizer_name == 'SGD':\n",
        "                optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "            elif optimizer_name == 'RMSprop':\n",
        "                optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
        "\n",
        "            # Train and evaluate\n",
        "            for epoch in range(num_epochs):\n",
        "                # Training loop\n",
        "                model.train()\n",
        "                running_loss = 0.0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "                for inputs, labels in train_loader:\n",
        "\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    running_loss += loss.item()\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (preds == labels).sum().item()\n",
        "                epoch_train_loss = running_loss / len(train_loader)\n",
        "                epoch_train_accuracy = correct / total\n",
        "                train_losses.append(epoch_train_loss)\n",
        "                train_accuracies.append(epoch_train_accuracy)\n",
        "\n",
        "                # Validation loop\n",
        "                model.eval()\n",
        "                all_preds = []\n",
        "                all_labels = []\n",
        "                running_loss = 0.0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "                with torch.no_grad():\n",
        "                   for inputs, labels in val_loader:\n",
        "\n",
        "                      inputs, labels = inputs.to(device), labels.to(device)\n",
        "                      outputs = model(inputs)\n",
        "                      loss = criterion(outputs, labels)\n",
        "                      running_loss += loss.item()\n",
        "                      _, preds = torch.max(outputs, 1)\n",
        "                      total += labels.size(0)\n",
        "                      correct += (preds == labels).sum().item()\n",
        "                      all_preds.extend(preds.cpu().numpy())\n",
        "                      all_labels.extend(labels.cpu().numpy())\n",
        "                epoch_val_loss = running_loss / len(val_loader)\n",
        "                epoch_val_accuracy = correct / total\n",
        "                val_losses.append(epoch_val_loss)\n",
        "                val_accuracies.append(epoch_val_accuracy)\n",
        "\n",
        "                # Calculate F1 score\n",
        "                f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "                #print(f\"Fold {fold+1}, Epoch {epoch+1}, F1 Score: {f1}\")\n",
        "                print(\"Fold \" + str(fold+1) + \", Epoch \" + str(epoch+1) + \", F1 Score: \" + str(f1))\n",
        "                # Update best F1 score and model\n",
        "                if f1 > best_f1:\n",
        "                  best_f1 = f1\n",
        "\n",
        "            f1_vals[fold] = best_f1\n",
        "            results.append({\n",
        "                    'model': 'ConvNeXt_small',\n",
        "                    'Optimizer': optimizer_name,\n",
        "                    'batch_size': batch_size,\n",
        "                    'learning_rate': lr,\n",
        "                    'Fold': fold+1,\n",
        "                    'F1 Score': best_f1\n",
        "                    })\n",
        "        results.append({\n",
        "                  'min f1': min(f1_vals),\n",
        "                  'max f1': max(f1_vals),\n",
        "                  'average f1': sum(f1_vals)/len(f1_vals)\n",
        "                  })\n",
        "\n",
        "        # Save the results as a DataFrame to be saved in an Excel file\n",
        "        df = pd.DataFrame(results)\n",
        "\n",
        "        # Load an existing Excel file\n",
        "        if os.path.isfile(results_directory):\n",
        "            existing_file = pd.read_excel(results_directory)\n",
        "            # Append the new DataFrame to the existing file\n",
        "            df = pd.concat([existing_file, df])\n",
        "\n",
        "        # Write the DataFrame to the excel file in the directory\n",
        "        df.to_excel(results_directory, index=False)\n",
        "# -------------- End of Save the results in a excel file ----------------\n",
        "\n"
      ],
      "metadata": {
        "id": "VgjPp4cMpk4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------- Save the results in a excel file ---------------------\n",
        "# Save the results as a DataFrame to be saved in an Excel file\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Load an existing Excel file\n",
        "if os.path.isfile(results_directory):\n",
        "  existing_file = pd.read_excel(results_directory)\n",
        "  # Append the new DataFrame to the existing file\n",
        "  df = pd.concat([existing_file, df])\n",
        "\n",
        "# Write the DataFrame to the excel file in the directory\n",
        "df.to_excel(results_directory, index=False)\n",
        "# -------------- End of Save the results in a excel file ----------------\n",
        "\n",
        "# -------------------- Plot the error/epoch plot ------------------------\n",
        "\n",
        "# Define the filename for the Excel file\n",
        "excel_filename = Plot_directory\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(os.path.dirname(excel_filename), exist_ok=True)\n",
        "\n",
        "# Check if the Excel file exists\n",
        "if os.path.exists(excel_filename):\n",
        "    # Load existing Excel file\n",
        "    wb = load_workbook(excel_filename)\n",
        "    ws = wb.active\n",
        "else:\n",
        "    # Create a new Excel workbook\n",
        "    wb = Workbook()\n",
        "    ws = wb.active"
      ],
      "metadata": {
        "id": "4-qvdKVbrR1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Tv9HiJyQRV5"
      },
      "source": [
        "# EfficientNetB0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# EfficientNetB0\"\"\"\n",
        "\n",
        "! pip install --upgrade efficientnet-pytorch\n",
        "\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "# An empty list for storing the results\n",
        "results = []\n",
        "f1_vals = [0] * k_folds\n",
        "results.append({'Model': 'EfficientNetB0'})\n",
        "\n",
        "# Initialize lists to store training and validation losses and accuracies\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "num_epochs = epochs\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for batch_size in batch_sizes:\n",
        "      for optimizer_name in optimizers:\n",
        "        print(f\"Training with optimizer: {optimizer_name}, learning rate: {lr}, batch size: {batch_size}\")\n",
        "        for fold, (train_idx, val_idx) in enumerate(skf.split(dataset.imgs, dataset.targets)):\n",
        "            train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
        "            val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
        "\n",
        "            train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "            val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
        "\n",
        "            best_f1 = 0\n",
        "            # Define the device (GPU if available, otherwise CPU)\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "            # Define the neural network\n",
        "            model = EfficientNet.from_pretrained('efficientnet-b0', num_classes).to(device)  # Move the model to GPU\n",
        "\n",
        "            # Define loss function\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "            # Define optimizer\n",
        "            if optimizer_name == 'Adam':\n",
        "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "            elif optimizer_name == 'SGD':\n",
        "                optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "            elif optimizer_name == 'RMSprop':\n",
        "                optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
        "\n",
        "            # Train and evaluate\n",
        "            for epoch in range(num_epochs):\n",
        "                # Training loop\n",
        "                model.train()\n",
        "                running_loss = 0.0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "                for inputs, labels in train_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device) # Move inputs and labels to GPU\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    running_loss += loss.item()\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (preds == labels).sum().item()\n",
        "                epoch_train_loss = running_loss / len(train_loader)\n",
        "                epoch_train_accuracy = correct / total\n",
        "                train_losses.append(epoch_train_loss)\n",
        "                train_accuracies.append(epoch_train_accuracy)\n",
        "\n",
        "                # Validation loop\n",
        "                model.eval()\n",
        "                all_preds = []\n",
        "                all_labels = []\n",
        "                running_loss = 0.0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "                with torch.no_grad():\n",
        "                  for inputs, labels in val_loader:\n",
        "                      inputs, labels = inputs.to(device), labels.to(device) # Move inputs and labels to GPU\n",
        "                      outputs = model(inputs)\n",
        "                      loss = criterion(outputs, labels)\n",
        "                      running_loss += loss.item()\n",
        "                      _, preds = torch.max(outputs, 1)\n",
        "                      total += labels.size(0)\n",
        "                      correct += (preds == labels).sum().item()\n",
        "                      all_preds.extend(preds.cpu().numpy())\n",
        "                      all_labels.extend(labels.cpu().numpy())\n",
        "                epoch_val_loss = running_loss / len(val_loader)\n",
        "                epoch_val_accuracy = correct / total\n",
        "                val_losses.append(epoch_val_loss)\n",
        "                val_accuracies.append(epoch_val_accuracy)\n",
        "\n",
        "                # Calculate F1 score\n",
        "                f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "                print(f\"Fold {fold+1}, Epoch {epoch+1}, F1 Score: {f1}\")\n",
        "\n",
        "                # Update best F1 score and model\n",
        "                if f1 > best_f1:\n",
        "                  best_f1 = f1\n",
        "\n",
        "            f1_vals[fold] = best_f1\n",
        "            results.append({\n",
        "                    'Optimizer': optimizer_name,\n",
        "                    'batch_size': batch_size,\n",
        "                    'learning_rate': lr,\n",
        "                    'Fold': fold+1,\n",
        "                    'F1 Score': best_f1\n",
        "              })\n",
        "        results.append({\n",
        "                  'min f1': min(f1_vals),\n",
        "                  'max f1': max(f1_vals),\n",
        "                  'average f1': sum(f1_vals)/len(f1_vals)\n",
        "                  })\n",
        "\n"
      ],
      "metadata": {
        "id": "4-rg5dJBp6ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ----------------- Save the results in a excel file ---------------------\n",
        "# Save the results as a DataFrame to be saved in an Excel file\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Load an existing Excel file\n",
        "if os.path.isfile(results_directory):\n",
        "  existing_file = pd.read_excel(results_directory)\n",
        "  # Append the new DataFrame to the existing file\n",
        "  df = pd.concat([existing_file, df])\n",
        "\n",
        "# Write the DataFrame to the excel file in the directory\n",
        "df.to_excel(results_directory, index=False)\n",
        "# -------------- End of Save the results in a excel file ----------------\n",
        "\n",
        "# -------------------- Plot the error/epoch plot ------------------------\n",
        "\n",
        "# Define the filename for the Excel file\n",
        "excel_filename = Plot_directory\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(os.path.dirname(excel_filename), exist_ok=True)\n",
        "\n",
        "# Check if the Excel file exists\n",
        "if os.path.exists(excel_filename):\n",
        "    # Load existing Excel file\n",
        "    wb = load_workbook(excel_filename)\n",
        "    ws = wb.active\n",
        "else:\n",
        "    # Create a new Excel workbook\n",
        "    wb = Workbook()\n",
        "    ws = wb.active"
      ],
      "metadata": {
        "id": "LIJHROAwqvY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x77YylbmXsd2"
      },
      "source": [
        "#Transformer: ViT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X84z7H_x4lsF"
      },
      "outputs": [],
      "source": [
        "# Define transformations by Composing several transforms together\n",
        "# and change th image_size to (224*224) to be compatible with ViT\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224*224),\n",
        "    #Convert a PIL Image or ndarray to tensor\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Use ImageFolder to load your dataset\n",
        "dataset = torchvision.datasets.ImageFolder(root = Preprocessed_DS_directory, transform=transform)\n",
        "\n",
        "# Define number of classes and list of labels in the dataset\n",
        "num_classes = len(dataset.classes)\n",
        "classes = dataset.classes\n",
        "\n",
        "# Initialize lists to store training and validation losses and accuracies\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"#Transformer: ViT\"\"\"\n",
        "\n",
        "from transformers import ViTModel, ViTConfig\n",
        "\n",
        "# Define transformations by Composing several transforms together\n",
        "# and change th image_size to (224*224) to be compatible with ViT\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224*224),\n",
        "    #Convert a PIL Image or ndarray to tensor\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Use ImageFolder to load your dataset\n",
        "dataset = torchvision.datasets.ImageFolder(root = Preprocessed_DS_directory, transform=transform)\n",
        "\n",
        "# Define number of classes and list of labels in the dataset\n",
        "num_classes = len(dataset.classes)\n",
        "classes = dataset.classes\n",
        "\n",
        "# An empty list for storing the results\n",
        "results = []\n",
        "f1_vals = [0] * k_folds\n",
        "results.append({'Model': 'Transformr: ViT'})\n",
        "\n",
        "# Initialize lists to store training and validation losses and accuracies\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "num_epochs = epochs\n",
        "\n",
        "for batch_size in batch_sizes:\n",
        "    for lr in learning_rates:\n",
        "      for optimizer_name in optimizers:\n",
        "        #print(f\"Training with optimizer: {optimizer_name}, learning rate: {lr}, batch size: {batch_size}\")\n",
        "        print(\"Training with optimizer: \" + optimizer_name + \", learning rate: \" + str(lr) + \", batch size: \" + str(batch_size))\n",
        "        for fold, (train_idx, val_idx) in enumerate(skf.split(dataset.imgs, dataset.targets)):\n",
        "            train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
        "            val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
        "\n",
        "            train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "            val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
        "\n",
        "            best_f1 = 0\n",
        "\n",
        "            # Define the device (GPU if available, otherwise CPU)\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "            # Define the neural network\n",
        "            config = ViTConfig.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "            model = ViTModel(config).to(device)  # Move the model to GPU\n",
        "\n",
        "            #model = ViTModel(config)\n",
        "\n",
        "            # Define loss function\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "            # Define optimizer\n",
        "            if optimizer_name == 'Adam':\n",
        "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "            elif optimizer_name == 'SGD':\n",
        "                optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "            elif optimizer_name == 'RMSprop':\n",
        "                optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
        "\n",
        "            # Train and evaluate\n",
        "            for epoch in range(num_epochs):\n",
        "                # Training loop\n",
        "                model.train()\n",
        "                running_loss = 0.0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "                for inputs, labels in train_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device) # Move inputs and labels to GPU\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    running_loss += loss.item()\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (preds == labels).sum().item()\n",
        "                epoch_train_loss = running_loss / len(train_loader)\n",
        "                epoch_train_accuracy = correct / total\n",
        "                train_losses.append(epoch_train_loss)\n",
        "                train_accuracies.append(epoch_train_accuracy)\n",
        "\n",
        "                # Validation loop\n",
        "                model.eval()\n",
        "                all_preds = []\n",
        "                all_labels = []\n",
        "                running_loss = 0.0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "                with torch.no_grad():\n",
        "                  for inputs, labels in val_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to GPU\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    running_loss += loss.item()\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    total += labels.size(0)\n",
        "                    correct += (preds == labels).sum().item()\n",
        "                    all_preds.extend(preds.cpu().numpy())\n",
        "                    all_labels.extend(labels.cpu().numpy())\n",
        "                epoch_val_loss = running_loss / len(val_loader)\n",
        "                epoch_val_accuracy = correct / total\n",
        "                val_losses.append(epoch_val_loss)\n",
        "                val_accuracies.append(epoch_val_accuracy)\n",
        "\n",
        "                # Calculate F1 score\n",
        "                f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "                #print(f\"Fold {fold+1}, Epoch {epoch+1}, F1 Score: {f1}\")\n",
        "                print(\"Fold \" + str(fold+1) + \", Epoch \" + str(epoch+1) + \", F1 Score: \" + str(f1))\n",
        "                # Update best F1 score and model\n",
        "                if f1 > best_f1:\n",
        "                  best_f1 = f1\n",
        "\n",
        "            f1_vals[fold] = best_f1\n",
        "            results.append({\n",
        "                    'Optimizer': optimizer_name,\n",
        "                    'batch_size': batch_size,\n",
        "                    'learning_rate': lr,\n",
        "                    'Fold': fold+1,\n",
        "                    'F1 Score': best_f1\n",
        "              })\n",
        "        results.append({\n",
        "                  'min f1': min(f1_vals),\n",
        "                  'max f1': max(f1_vals),\n",
        "                  'average f1': sum(f1_vals)/len(f1_vals)\n",
        "                  })\n",
        "\n",
        "        # Define the filename for the Excel file\n",
        "        excel_filename = Plot_directory\n",
        "\n",
        "        # Create the directory if it doesn't exist\n",
        "        os.makedirs(os.path.dirname(excel_filename), exist_ok=True)\n",
        "\n",
        "        # Check if the Excel file exists\n",
        "        if os.path.exists(excel_filename):\n",
        "            # Load existing Excel file\n",
        "            wb = load_workbook(excel_filename)\n",
        "            ws = wb.active\n",
        "        else:\n",
        "            # Create a new Excel workbook\n",
        "            wb = Workbook()\n",
        "            ws = wb.active\n"
      ],
      "metadata": {
        "id": "rNQ3DUtCqLUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "        # Save the results as a DataFrame to be saved in an Excel file\n",
        "        df = pd.DataFrame(results)\n",
        "\n",
        "        # Load an existing Excel file\n",
        "        if os.path.isfile(results_directory):\n",
        "            existing_file = pd.read_excel(results_directory)\n",
        "            # Append the new DataFrame to the existing file\n",
        "            df = pd.concat([existing_file, df])\n",
        "\n",
        "        # Write the DataFrame to the excel file in the directory\n",
        "        df.to_excel(results_directory, index=False)\n",
        "# -------------------- Plot the error/epoch plot ------------------------\n",
        "\n",
        "# Define the filename for the Excel file\n",
        "excel_filename = Plot_directory\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(os.path.dirname(excel_filename), exist_ok=True)\n",
        "\n",
        "# Check if the Excel file exists\n",
        "if os.path.exists(excel_filename):\n",
        "    # Load existing Excel file\n",
        "    wb = load_workbook(excel_filename)\n",
        "    ws = wb.active\n",
        "else:\n",
        "    # Create a new Excel workbook\n",
        "    wb = Workbook()\n",
        "    ws = wb.active"
      ],
      "metadata": {
        "id": "jLgDi1oTq8Yy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "iNW54ARY62Mb"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
